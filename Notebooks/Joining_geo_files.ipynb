{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd \n",
    "import geopandas as gpd\n",
    "\n",
    "import h3 # h3 bins from uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crash_df(train_file = '../Inputs/Train.csv'):  \n",
    "    crash_df = pd.read_csv(train_file, parse_dates=['datetime'])\n",
    "    return crash_df\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    dict_windows = {1: \"00-03\", 2: \"03-06\", 3: \"06-09\", 4: \"09-12\", 5: \"12-15\", 6: \"15-18\", 7: \"18-21\", 8: \"21-24\"}\n",
    "    dict_months = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "               7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
    "    \n",
    "    df[\"time_window\"] = df[\"datetime\"].apply(lambda x: math.floor(x.hour / 3) + 1)\n",
    "    df[\"time_window_str\"] = df[\"time_window\"].apply(lambda x: dict_windows.get(x))\n",
    "    df[\"day\"] = df[\"datetime\"].apply(lambda x: x.day)\n",
    "    df[\"month\"] = df[\"datetime\"].apply(lambda x: dict_months.get(x.month))\n",
    "    df[\"year\"] = df[\"datetime\"].apply(lambda x: x.year)\n",
    "    df[\"weekday\"] = df[\"datetime\"].apply(lambda x: x.weekday())\n",
    "    return df\n",
    "\n",
    "def assign_hex_bin(df,lat_column=\"latitude\",lon_column=\"longitude\"):\n",
    "    df[\"h3_zone_5\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 5),axis=1)\n",
    "    df[\"h3_zone_6\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 6),axis=1)\n",
    "    df[\"h3_zone_7\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 7),axis=1)\n",
    "    return df\n",
    "\n",
    "def export_df_to_csv(df,path_file='../Inputs/train_h3.csv'):\n",
    "    df.to_csv(path_file,index=False)\n",
    "    print(f'file created {path_file}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create command line commands for downlaoding uber movement data with OSM segment info\n",
    "month_list = [('01','31'),\n",
    "              ('02','28'),\n",
    "              ('03','31'),\n",
    "              ('04','30'),\n",
    "              ('05','31'),\n",
    "              ('06','30'),\n",
    "              ('07','31'),\n",
    "              ('08','31'),\n",
    "              ('09','30'),\n",
    "              ('10','31'),\n",
    "              ('11','30'),\n",
    "              ('12','31')]\n",
    "for year in ['2018','2019']:\n",
    "    for month, end_day in month_list:\n",
    "        break # remove when you want the commands\n",
    "        # print([f'mdt speeds-to-geojson nairobi {year}-{month}-01 {year}-{month}-{end_day} --output=Inputs/nairobi_{year}_{month}geojson.geojson'])\n",
    "        # print([f'mdt speeds-transform historical nairobi {year}-{month}-1 {year}-{month}-{end_day} --output=Inputs/nairobi_{year}_{month}_osm.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_segment_files(path='../Inputs/', road_surveys='Segment_info.csv',segments_geometry='segments_geometry.geojson'):\n",
    "    ''' \n",
    "        Load the survey data, Load the segment geometry, Join the two segment dfs.\n",
    "        return a combined dataframe\n",
    "    '''\n",
    "    road_surveys = pd.read_csv(path+road_surveys)\n",
    "    road_segment_locs = gpd.read_file(path+segments_geometry)\n",
    "    segments_merged = pd.merge(road_segment_locs, road_surveys, on='segment_id', how='left')\n",
    "    segments_merged[\"longitude\"] = segments_merged.geometry.centroid.x\n",
    "    segments_merged[\"latitude\"] = segments_merged.geometry.centroid.y\n",
    "    segments_merged = assign_hex_bin(segments_merged)\n",
    "    return segments_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_df = create_crash_df(train_file = '../Inputs/Train.csv')\n",
    "crash_df = create_temporal_features(crash_df)\n",
    "crash_df = assign_hex_bin(crash_df)\n",
    "#crash_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_merged = join_segment_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs work\n",
    "segments_h3_zone_7= segments_merged.groupby(by='h3_zone_7').max()\n",
    "segments_h3_zone_7['h3_zone_5']= segments_merged.groupby(by='h3_zone_5').latitude.max()\n",
    "segments_h3_zone_7['h3_zone_6']= segments_merged.groupby(by='h3_zone_6').latitude.max()\n",
    "segments_h3_zone_7['latitude']= segments_merged.groupby(by='h3_zone_7').latitude.mean()\n",
    "segments_h3_zone_7['longitude']= segments_merged.groupby(by='h3_zone_7').longitude.mean()\n",
    "segments_h3_zone_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Inputs/'\n",
    "road_surveys='Segment_info.csv'\n",
    "segments_geometry='segments_geometry.geojson'\n",
    "road_segment_locs = gpd.read_file(path+segments_geometry)\n",
    "road_surveys = pd.read_csv(path+road_surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_segment_locs.segment_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_surveys.segment_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_segment_crash_files(crash_data=crash_df, segments=segments_merged, h3_zone='h3_zone_5'):\n",
    "    ''' \n",
    "        Combine the segment data and the crash data by chosen hex.\n",
    "        return a combined dataframe\n",
    "    '''\n",
    "    # Add some groupby function here\n",
    "    segment_crash_df = pd.merge(crash_data, segments, on=h3_zone, how='left')\n",
    "    return segment_crash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_crash_df = join_segment_crash_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_crash_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The crash data and the segment data needs to be grouped before this join makes sense\n",
    "### Also need to deal with the issue of missing segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_movement_osm = pd.read_csv('../Inputs/nairobi_2018_01_osm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_movement_osm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojsonfile = gpd.read_file('../Inputs/nairobi_2018_01_speeds.geojson', parse_dates=['utc_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojsonfile.osmhighway.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojsonfile.speed_mean_kph.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojsonfile.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nairobi_ambulance] *",
   "language": "python",
   "name": "conda-env-nairobi_ambulance-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
