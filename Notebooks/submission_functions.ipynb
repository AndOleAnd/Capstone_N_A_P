{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "import geopandas as gpd\n",
    "import h3 # h3 bins from uber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import holidays\n",
    "#pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crash_df(train_file = '../Inputs/Train.csv'):  \n",
    "    '''\n",
    "    loads crash data from input folder into dataframe\n",
    "    '''\n",
    "    crash_df = pd.read_csv(train_file, parse_dates=['datetime'])\n",
    "    return crash_df\n",
    "    \n",
    "def create_temporal_features(df, date_column='datetime'):\n",
    "    '''\n",
    "    Add the set of temporal features the the df based on the datetime column. Returns the dataframe.\n",
    "    '''\n",
    "    dict_windows = {1: \"00-03\", 2: \"03-06\", 3: \"06-09\", 4: \"09-12\", 5: \"12-15\", \n",
    "                    6: \"15-18\", 7: \"18-21\", 8: \"21-24\"}\n",
    "    dict_months = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "                   7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
    "    rainy_season = [\"Mar\", \"Apr\", \"May\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    df[\"time\"] = df[date_column].apply(lambda x: x.time())\n",
    "    df[\"time_window\"] = df[date_column].apply(lambda x: math.floor(x.hour / 3) + 1)\n",
    "    df[\"time_window_str\"] = df[\"time_window\"].apply(lambda x: dict_windows.get(x))\n",
    "    df[\"day\"] = df[date_column].apply(lambda x: x.day)\n",
    "    df[\"weekday\"] = df[date_column].apply(lambda x: x.weekday())\n",
    "    df[\"month\"] = df[date_column].apply(lambda x: dict_months.get(x.month))\n",
    "    df[\"half_year\"] = df[date_column].apply(lambda x: 1 if x.month<7 else 2)\n",
    "    df[\"rainy_season\"] = df[\"month\"].apply(lambda x: 1 if (x in rainy_season) else 0)\n",
    "    df[\"year\"] = df[date_column].apply(lambda x: x.year)\n",
    "    df[\"date_trunc\"] = df[date_column].apply(lambda x: x.date()) #this does something strange that breaks the code if higher\n",
    "    df[\"holiday\"] = df[\"date_trunc\"].apply(lambda x: 1 if (x in holidays.Kenya()) else 0)\n",
    "    return df\n",
    "\n",
    "def drop_temporal(df):\n",
    "    '''\n",
    "    helper function to remove all the granular temporal columns once they have been used for generating other columns for joining.\n",
    "    '''\n",
    "    df = df.drop([\"day\", \"time_window\", \"time_window_str\", \"time_window_str\", \"month\", \"year\", \"weekday\", \"rainy_season\", \"date_trunc\", \"time\", \"half_year\", \"holiday\"], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def assign_hex_bin(df,lat_column=\"latitude\",lon_column=\"longitude\"):\n",
    "    '''\n",
    "    Takes lat,lon and creates column with h3 bin name for three levels of granualirity.\n",
    "    '''\n",
    "    df[\"h3_zone_5\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 5),axis=1)\n",
    "    df[\"h3_zone_6\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 6),axis=1)\n",
    "    df[\"h3_zone_7\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 7),axis=1)\n",
    "    return df\n",
    "\n",
    "def plot_centroids(crash_data_df, centroids, cluster='cluster'):\n",
    "    '''\n",
    "    plots the crash data points from crash_data_df and overlays the ambulance location from centroids. \n",
    "    Can be used in a loop by giving 'cluster' value as a parameter to label the chart with the cluster name.\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(8, 5))\n",
    "    plt.scatter(x = crash_data_df['longitude'], y=crash_data_df['latitude'], s=1, label='Crash locations' )\n",
    "    plt.scatter(x = centroids[:,1], y=centroids[:,0], marker=\"x\",\n",
    "                color='r',label='Ambulances locations',s=100)\n",
    "    axs.set_title('Scatter plot : Ambulaces locations vs Crash locations :'+cluster)\n",
    "    plt.xlabel(\"latitude\")\n",
    "    plt.ylabel(\"longitude\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_k_means(crash_df_with_cluster):\n",
    "    '''\n",
    "    Runs mulitple k-means clustering on the crash_df based on the split of tw clusters in the 'cluster' column.\n",
    "    Feeds the crash data points and the resulting centroids for that cluster into the plot centroids function.\n",
    "    return once chart for each cluster.\n",
    "    '''\n",
    "    for i in crash_df_with_cluster.cluster.unique():\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-3, max_iter=100, n_init=20, copy_x= True)\n",
    "        kmeans.fit(crash_df_with_cluster.query('cluster==@i')[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        plot_centroids(crash_df_with_cluster.query('cluster==@i'), centroids=centroids, cluster=i) \n",
    "        \n",
    "def plot_dendrogram(df):\n",
    "    '''Use Dendrogram to determine an optimal number of clusters'''\n",
    "    plt.figure(figsize=(45,18))\n",
    "    plt.title('Androgram')\n",
    "    plt.xlabel('time_buckets_days')\n",
    "    plt.ylabel('Euclidean distances')\n",
    "    dendrogram = sch.dendrogram(sch.linkage(df, method = 'ward'))\n",
    "    plt.show()\n",
    "\n",
    "def calculate_TW_cluster(crash_df, method='MeanShift', verbose=0):\n",
    "    '''\n",
    "    Takes crash dataframe with temporal features added as input\n",
    "    Function to perform clustering of time windows and assign labels back to crash dataframe. \n",
    "    Output is dataframe with additional column for labels\n",
    "    If verbosity is increased, information about the clusters to printed.\n",
    "    '''\n",
    "    group_stats = crash_df.groupby(['time_window_str', 'weekday'])\n",
    "    group_stats = group_stats.agg({'latitude': [np.mean, np.std],'longitude': [np.mean, np.std, 'count']})\n",
    "    # flatten out groupby object and name columns again\n",
    "    group_stats = group_stats.reset_index()\n",
    "    group_stats.columns = group_stats.columns.get_level_values(0)\n",
    "    group_stats.columns.values[[2,3,4,5,6]] = ['latitude_mean', 'latitude_std',\n",
    "                                               'longitude_mean', 'longitude_std', 'RTA_count']\n",
    "    X = group_stats.loc[:,['RTA_count']]#, 'latitude_mean', 'latitude_std','longitude_mean', 'longitude_std']]\n",
    "    scaler = StandardScaler()\n",
    "    scale_columns = ['latitude_mean', 'latitude_std','longitude_mean', 'longitude_std']\n",
    "    #X[scale_columns] = scaler.fit_transform(X[scale_columns])\n",
    "    if verbose > 5:\n",
    "        X1 = X.copy()\n",
    "        X1['RTA_count'] = minmax_scale(X1['RTA_count'])\n",
    "        plot_dendrogram(X1)\n",
    "        \n",
    "    if method == 'MeanShift':\n",
    "        #X['RTA_count'] = minmax_scale(X['RTA_count'])\n",
    "        ms_model = MeanShift().fit(X)\n",
    "        labels = ms_model.labels_\n",
    "\n",
    "    elif method == 'GMM':\n",
    "        X['RTA_count'] = minmax_scale(X['RTA_count'])\n",
    "        gmm = GaussianMixture(n_components=4, verbose=verbose, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        labels = gmm.predict(X)\n",
    "    else:\n",
    "        display('Select method \"MeanShift\" or \"GMM\"')\n",
    "        #return 'error'\n",
    "\n",
    "    labels = pd.DataFrame(labels,columns=['cluster'])\n",
    "    clustered_time_buckets = pd.concat([group_stats,labels], axis=1)\n",
    "\n",
    "    if verbose > 0:\n",
    "        display(clustered_time_buckets.groupby('cluster').agg({'RTA_count': ['count', np.sum]}))\n",
    "    if verbose > 1:\n",
    "        plot_TW_cluster(clustered_time_buckets)\n",
    "    \n",
    "    crash_df = crash_df.merge(clustered_time_buckets[['time_window_str', 'weekday','cluster']], how='left', on=['time_window_str', 'weekday'])\n",
    "    return crash_df\n",
    "\n",
    "def plot_TW_cluster(clustered_time_buckets):\n",
    "    '''\n",
    "    Displays stripplot to show how different times of the week are assigned to TW clusters.\n",
    "    '''\n",
    "    tb_clusters = sns.FacetGrid(clustered_time_buckets,hue='cluster', height=5)\n",
    "    tb_clusters.map(sns.stripplot,'weekday', 'time_window_str', s=25, order = ['00-03', '03-06', '06-09', '09-12', \n",
    "                                                                              '12-15', '15-18', '18-21', '21-24'])\n",
    "    \n",
    "def assign_TW_cluster(weekday, time_window, holiday=0, strategy='baseline'):\n",
    "    '''\n",
    "    Can be used in a lambda function to return the time window cluster for a given day and time window.\n",
    "    e.g. crash_df[\"cluster\"] = crash_df.apply(lambda x: return_TW_cluster(x.weekday, x.time_window_str) ,axis=1)\n",
    "    This is called by the function: create_cluster_feature.\n",
    "    '''\n",
    "    if strategy == 'baseline':\n",
    "        return 'baseline'\n",
    "    \n",
    "    if strategy == 'mean_shift_modified':\n",
    "        if holiday == 1:\n",
    "            return 'off_peak'        \n",
    "        elif weekday == 6:\n",
    "            return 'off_peak'\n",
    "        elif weekday in [0,1,2,3,4]:\n",
    "            if time_window in [\"06-09\"]:\n",
    "                return 'peak'\n",
    "            elif time_window in [\"09-12\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'    \n",
    "        elif weekday == 5:\n",
    "            if time_window in [\"06-09\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'\n",
    "            elif time_window in [\"09-12\"]:\n",
    "                return 'peak'\n",
    "    \n",
    "    elif strategy == 'saturday_2':\n",
    "        if holiday == 1:\n",
    "            return 'off_peak'        \n",
    "        elif weekday == 6:\n",
    "            return 'off_peak'\n",
    "        elif weekday in [0,1,2,3,4]:\n",
    "            if time_window in [\"06-09\"]:\n",
    "                return 'peak'\n",
    "            elif time_window in [\"09-12\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'    \n",
    "        elif weekday == 5:\n",
    "            if time_window in [\"06-09\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'saturday_busy'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'\n",
    "            elif time_window in [\"09-12\"]:\n",
    "                return 'saturday_busy'      \n",
    "    \n",
    "    elif strategy == 'no_cluster':\n",
    "        return (str(weekday)+str(time_window)+str(holiday))\n",
    "            \n",
    "def create_cluster_feature(crash_df, strategy='baseline', verbose=0):\n",
    "    '''\n",
    "    Function takes crash df and creates new column with tw cluster labels.\n",
    "    If verbose is increased, the time window clusters will be visualised.\n",
    "    '''\n",
    "    crash_df[\"cluster\"] = crash_df.apply(lambda x: \n",
    "                                         assign_TW_cluster(weekday=x.weekday,\n",
    "                                                           time_window=x.time_window_str,\n",
    "                                                           strategy=strategy) \n",
    "                                         ,axis=1)\n",
    "    \n",
    "    print(f'{crash_df.cluster.nunique()} clusters created')\n",
    "    if verbose > 1:\n",
    "        tb_clusters = sns.FacetGrid(crash_df,hue='cluster', height=5)\n",
    "        tb_clusters.map(sns.stripplot,'weekday', 'time_window_str', s=20, \n",
    "                                       order = ['00-03', '03-06', '06-09', '09-12', \n",
    "                                                '12-15', '15-18', '18-21', '21-24'],\n",
    "                                    label = 'Time Window Clusters')\n",
    "    return crash_df\n",
    "            \n",
    "\n",
    "def create_baseline_submission_df(crash_data_df, date_start='2019-07-01', date_end='2020-01-01', method='k_means'):\n",
    "    '''Takes crash data and creates a data frame in the format needed for submission'''\n",
    "       \n",
    "    # star grid\n",
    "    if method == 'star':\n",
    "        lat_centroid = list(crash_data_df.latitude.quantile(q=[1/5,2/5,3/5,4/5]))\n",
    "        lon_centroid = list(crash_data_df.longitude.quantile(q=[1/4,2/4,3/4]))\n",
    "        centroids=[(lat_centroid[1],lon_centroid[0]),(lat_centroid[2],lon_centroid[0]),\n",
    "                   (lat_centroid[0],lon_centroid[1]),(lat_centroid[3],lon_centroid[1]),\n",
    "                   (lat_centroid[1],lon_centroid[2]),(lat_centroid[2],lon_centroid[2])]\n",
    "    \n",
    "    # Create centroids via k-means\n",
    "    if method == 'k_means':\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-3, max_iter=100, n_init=20, copy_x= True)\n",
    "        kmeans.fit(crash_data_df[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Create Date range covering submission period set\n",
    "    dates = pd.date_range(date_start, date_end, freq='3h')\n",
    "        \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({'date':dates})\n",
    "    for ambulance in range(6):\n",
    "        # Place an ambulance in the center of the city:\n",
    "        submission_df['A'+str(ambulance)+'_Latitude'] = centroids[ambulance][0]\n",
    "        submission_df['A'+str(ambulance)+'_Longitude'] = centroids[ambulance][1]\n",
    "    return submission_df, centroids\n",
    "\n",
    "def create_k_means_centroids(crash_df_with_cluster, verbose=0):\n",
    "    centroids_dict = {}\n",
    "    for i in crash_df_with_cluster.cluster.unique():\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-4, max_iter=200, n_init=20)\n",
    "        kmeans.fit(crash_df_with_cluster.query('cluster==@i')[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        centroids_dict[i] = centroids.flatten()\n",
    "        if verbose > 2:\n",
    "            plot_centroids(crash_df_with_cluster.query('cluster==@i'), centroids, cluster=i)\n",
    "        if verbose > 5:\n",
    "            print(centroids)\n",
    "    print(f'{len(centroids_dict)} centroids created')\n",
    "    return centroids_dict\n",
    "        \n",
    "def centroid_to_submission(centroids_dict, date_start='2019-07-01', date_end='2020-01-01', tw_cluster_strategy='baseline'):\n",
    "    '''Takes dictionary of clusters and centroids and creates a data frame in the format needed for submission'''\n",
    "\n",
    "    # Create Date range covering submission period set\n",
    "    dates = pd.date_range(date_start, date_end, freq='3h')\n",
    "    submission_df = pd.DataFrame({'date':dates})\n",
    "    submission_df = create_temporal_features(submission_df,'date')\n",
    "    submission_df[\"cluster\"] = submission_df.apply(lambda x: assign_TW_cluster(x.weekday, x.time_window_str, strategy=tw_cluster_strategy) ,axis=1)\n",
    "    ambulance_columns = ['A0_Latitude', 'A0_Longitude', 'A1_Latitude','A1_Longitude', 'A2_Latitude', 'A2_Longitude', \n",
    "                         'A3_Latitude', 'A3_Longitude', 'A4_Latitude', 'A4_Longitude', 'A5_Latitude', 'A5_Longitude']\n",
    "    for i in submission_df[\"cluster\"].unique():\n",
    "        submission_df[\"placements\"] = submission_df[\"cluster\"].apply(lambda x: centroids_dict.get(x))\n",
    "        submission_df[ambulance_columns] = pd.DataFrame(submission_df.placements.tolist(), index=submission_df.index)\n",
    "    submission_df = submission_df.drop('placements', axis=1)\n",
    "    submission_df = drop_temporal(submission_df)\n",
    "    submission_df = submission_df.drop([\"cluster\"], axis=1)\n",
    "    print('submission dataframe created')\n",
    "    return submission_df\n",
    "\n",
    "def create_submission_csv(submission_df, crash_source, tw_cluster_strategy, model_name, path='../Outputs/'):\n",
    "    '''Takes dataframe in submission format and outputs a csv file with matching name'''\n",
    "    current_time = datetime.datetime.now()\n",
    "    filename = f'{current_time.year}{current_time.month}{current_time.day}_{crash_source}_{tw_cluster_strategy}_{model_name}.csv'\n",
    "    submission_df.to_csv(path+filename,index=False)\n",
    "    print(f'{filename} saved in {path}') \n",
    "    \n",
    "    \n",
    "def score(train_placements_df, crash_df, test_start_date='2018-01-01', test_end_date='2019-12-31'):\n",
    "          \n",
    "    '''\n",
    "    Can be used to score the ambulance placements against a set of crashes. Can be used on all crash data, train_df or holdout_df as crash_df.\n",
    "    '''\n",
    "    test_df = crash_df.loc[(crash_df.datetime > test_start_date) & (crash_df.datetime < test_end_date)]\n",
    "    print(f'Data points in test period: {test_df.shape[0]}' )\n",
    "    total_distance = 0\n",
    "    for crash_date, c_lat, c_lon in test_df[['datetime', 'latitude', 'longitude']].values:\n",
    "        row = train_placements_df.loc[train_placements_df.date < crash_date].tail(1)\n",
    "        dists = []\n",
    "        for a in range(6):\n",
    "            dist = ((c_lat - row[f'A{a}_Latitude'].values[0])**2+(c_lon - row[f'A{a}_Longitude'].values[0])**2)**0.5 \n",
    "            dists.append(dist)\n",
    "        total_distance += min(dists)\n",
    "    return total_distance\n",
    "\n",
    "def split_accident_df(data, strategy, test_size=0.3, random_state=42):\n",
    "    '''\n",
    "    Splits the data set into a train and a test set.\n",
    "    strategy:\n",
    "        random = splits off random indices, using test_size and random_state parameters\n",
    "        year_2019 = splits the days of 2019 off into a test set\n",
    "        percentage_month = splits off the last days of every month to the test set according to the test_size\n",
    "        2nd_half_2018 = oversamples the months from July to December 2018 by about 33%\n",
    "    '''\n",
    "    if strategy == \"random\":\n",
    "        data = data.sample(frac=1, random_state=random_state).reset_index().drop(\"index\", axis=1)\n",
    "        split_at = round(data.shape[0] * test_size)\n",
    "        data_train = data.iloc[split_at:, :]\n",
    "        data_test = data.iloc[:split_at, :]\n",
    "    elif strategy == \"year_2019\":\n",
    "        data_train = data[data[\"datetime\"] < \"2019-01-01\"]\n",
    "        data_test = data[data[\"datetime\"] >= \"2019-01-01\"]\n",
    "    elif strategy == \"percentage_month\":\n",
    "        split_at = round(30 * (1-test_size))\n",
    "        data_train = data.loc[data[\"day\"] <= split_at]\n",
    "        data_test = data.loc[data[\"day\"] > split_at]\n",
    "    elif strategy == \"2nd_half_2018\":\n",
    "        train_samples = round(data.shape[0] * (1-test_size))\n",
    "        test_samples = round(data.shape[0] * test_size)\n",
    "        data_train = data.sample(n=train_samples, weights=\"half_year\", random_state=random_state)\n",
    "        data_test = data.sample(n=test_samples, weights=\"half_year\", random_state=random_state)\n",
    "        \n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ambulance_placement_pipeline(input_path='../Inputs/', output_path='../Outputs/', crash_source_csv='Train',\n",
    "                                 holdout_strategy='year_2019', holdout_test_size=0.3,\n",
    "                                 test_period_date_start='2019-01-01', test_period_date_end='2019-07-01',\n",
    "                                 tw_cluster_strategy='saturday_2', placement_model='k_means', verbose=0):  \n",
    "    '''\n",
    "    load crash data (from train or prediction) and apply feautre engineering, run tw clustering (based on strategy choice) \n",
    "    create ambulance placements, create output file.\n",
    "    '''\n",
    "    crash_df = create_crash_df(train_file = input_path+crash_source_csv+'.csv')\n",
    "    crash_df = create_temporal_features(crash_df)\n",
    "    crash_df, test_df = split_accident_df(data=crash_df, strategy=holdout_strategy, test_size=holdout_test_size)\n",
    "    test_df = drop_temporal(test_df)\n",
    "    crash_df = create_cluster_feature(crash_df, strategy=tw_cluster_strategy, verbose=verbose)\n",
    "    centroids_dict = create_k_means_centroids(crash_df, verbose=verbose)\n",
    "    train_placements_df = centroid_to_submission(centroids_dict, date_start='2018-01-01', date_end='2019-12-31', tw_cluster_strategy=tw_cluster_strategy)\n",
    "    print(f'Total size of test set: {test_df.shape[0]}')\n",
    "    test_score = score(train_placements_df, test_df, test_start_date=test_period_date_start, test_end_date=test_period_date_end)\n",
    "    print(f'Total size of train set: {crash_df.shape[0]}')\n",
    "    train_score = score(train_placements_df,crash_df, test_start_date=test_period_date_start, test_end_date=test_period_date_end)\n",
    "    print(f'Score on test set: {test_score / max(test_df.shape[0],1)}')\n",
    "    print(f'Score on train set: {train_score / crash_df.shape[0] } (avg distance per accident)')\n",
    "\n",
    "    \n",
    "# create file for submitting to zindi\n",
    "    submission_df = centroid_to_submission(centroids_dict, date_start='2019-07-01', date_end='2020-01-01', tw_cluster_strategy=tw_cluster_strategy)\n",
    "    create_submission_csv(submission_df, crash_source=crash_source_csv, tw_cluster_strategy=tw_cluster_strategy, model_name=placement_model, path=output_path)\n",
    "    \n",
    "ambulance_placement_pipeline(input_path='../Inputs/', output_path='../Outputs/', crash_source_csv='Train',\n",
    "                              holdout_strategy='random', holdout_test_size=0.2,\n",
    "                              test_period_date_start='2018-01-01', test_period_date_end='2019-12-31',\n",
    "                              tw_cluster_strategy='saturday_2', placement_model='k_means', verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nairobi_ambulance] *",
   "language": "python",
   "name": "conda-env-nairobi_ambulance-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
