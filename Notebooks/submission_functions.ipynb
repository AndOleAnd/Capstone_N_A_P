{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "import geopandas as gpd\n",
    "import h3 # h3 bins from uber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "import scipy.cluster.hierarchy as sch\n",
    "#pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crash_df(train_file = '../Inputs/Train.csv'):  \n",
    "    '''\n",
    "    loads crash data from input folder into dataframe\n",
    "    '''\n",
    "    crash_df = pd.read_csv(train_file, parse_dates=['datetime'])\n",
    "    return crash_df\n",
    "    \n",
    "def create_temporal_features(df, date_column='datetime'):\n",
    "    '''\n",
    "    Add the set of temporal features the the df based on the datetime column. Returns the dataframe.\n",
    "    '''\n",
    "    dict_windows = {1: \"00-03\", 2: \"03-06\", 3: \"06-09\", 4: \"09-12\", 5: \"12-15\", \n",
    "                    6: \"15-18\", 7: \"18-21\", 8: \"21-24\"}\n",
    "    dict_months = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "                   7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
    "    rainy_season = [\"Mar\", \"Apr\", \"May\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "    df[\"time_window\"] = df[date_column].apply(lambda x: math.floor(x.hour / 3) + 1)\n",
    "    df[\"time_window_str\"] = df[\"time_window\"].apply(lambda x: dict_windows.get(x))\n",
    "    df[\"day\"] = df[date_column].apply(lambda x: x.day)\n",
    "    df[\"month\"] = df[date_column].apply(lambda x: dict_months.get(x.month))\n",
    "    df[\"year\"] = df[date_column].apply(lambda x: x.year)\n",
    "    df[\"weekday\"] = df[date_column].apply(lambda x: x.weekday())\n",
    "    df[\"rainy_season\"] = df[\"month\"].apply(lambda x: 1 if (x in rainy_season) else 0)\n",
    "    return df\n",
    "\n",
    "def drop_temporal(df):\n",
    "    '''\n",
    "    helper function to remove all the granular temporal columns once they have been used for generating other columns for joining.\n",
    "    '''\n",
    "    df = df.drop([\"day\", \"time_window\",\"time_window_str\",\"time_window_str\",\"month\", \"year\", \"weekday\", \"rainy_season\", \"cluster\"], axis=1)\n",
    "    return df\n",
    "\n",
    "def assign_hex_bin(df,lat_column=\"latitude\",lon_column=\"longitude\"):\n",
    "    '''\n",
    "    Takes lat,lon and creates column with h3 bin name for three levels of granualirity.\n",
    "    '''\n",
    "    df[\"h3_zone_5\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 5),axis=1)\n",
    "    df[\"h3_zone_6\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 6),axis=1)\n",
    "    df[\"h3_zone_7\"] = df.apply(lambda x: h3.geo_to_h3(x[lat_column], x[lon_column], 7),axis=1)\n",
    "    return df\n",
    "\n",
    "def plot_centroids(crash_data_df, centroids, cluster='cluster'):\n",
    "    '''\n",
    "    plots the crash data points from crash_data_df and overlays the ambulance location from centroids. \n",
    "    Can be used in a loop by giving 'cluster' value as a parameter to label the chart with the cluster name.\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(8, 5))\n",
    "    plt.scatter(x = crash_data_df['longitude'], y=crash_data_df['latitude'], s=1, label='Crash locations' )\n",
    "    plt.scatter(x = centroids[:,1], y=centroids[:,0], marker=\"x\",\n",
    "                color='r',label='Ambulances locations',s=100)\n",
    "    axs.set_title('Scatter plot : Ambulaces locations vs Crash locations :'+cluster)\n",
    "    plt.xlabel(\"latitude\")\n",
    "    plt.ylabel(\"longitude\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_k_means(crash_df_with_cluster):\n",
    "    '''\n",
    "    Runs mulitple k-means clustering on the crash_df based on the split of tw clusters in the 'cluster' column.\n",
    "    Feeds the crash data points and the resulting centroids for that cluster into the plot centroids function.\n",
    "    return once chart for each cluster.\n",
    "    '''\n",
    "    for i in crash_df_with_cluster.cluster.unique():\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-3, max_iter=100, n_init=20, copy_x= True)\n",
    "        kmeans.fit(crash_df_with_cluster.query('cluster==@i')[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        plot_centroids(crash_df_with_cluster.query('cluster==@i'), centroids=centroids) \n",
    "        \n",
    "def plot_dendrogram(df):\n",
    "    '''Use Dendrogram to determine an optimal number of clusters'''\n",
    "    plt.figure(figsize=(45,18))\n",
    "    plt.title('Androgram')\n",
    "    plt.xlabel('time_buckets_days')\n",
    "    plt.ylabel('Euclidean distances')\n",
    "    dendrogram = sch.dendrogram(sch.linkage(df, method = 'ward'))\n",
    "    plt.show()\n",
    "\n",
    "def calculate_TW_cluster(crash_df, method='MeanShift', verbose=0):\n",
    "    '''\n",
    "    Takes crash dataframe with temporal features added as input\n",
    "    Function to perform clustering of time windows and assign labels back to crash dataframe. \n",
    "    Output is dataframe with additional column for labels\n",
    "    If verbosity is increased, information about the clusters to printed.\n",
    "    '''\n",
    "    group_stats = crash_df.groupby(['time_window_str', 'weekday'])\n",
    "    group_stats = group_stats.agg({'latitude': [np.mean, np.std],'longitude': [np.mean, np.std, 'count']})\n",
    "    # flatten out groupby object and name columns again\n",
    "    group_stats = group_stats.reset_index()\n",
    "    group_stats.columns = group_stats.columns.get_level_values(0)\n",
    "    group_stats.columns.values[[2,3,4,5,6]] = ['latitude_mean', 'latitude_std',\n",
    "                                               'longitude_mean', 'longitude_std', 'RTA_count']\n",
    "    X = group_stats.loc[:,['RTA_count']]#, 'latitude_mean', 'latitude_std','longitude_mean', 'longitude_std']]\n",
    "    scaler = StandardScaler()\n",
    "    scale_columns = ['latitude_mean', 'latitude_std','longitude_mean', 'longitude_std']\n",
    "    #X[scale_columns] = scaler.fit_transform(X[scale_columns])\n",
    "    if verbose > 5:\n",
    "        X1 = X.copy()\n",
    "        X1['RTA_count'] = minmax_scale(X1['RTA_count'])\n",
    "        plot_dendrogram(X1)\n",
    "        \n",
    "    if method == 'MeanShift':\n",
    "        #X['RTA_count'] = minmax_scale(X['RTA_count'])\n",
    "        ms_model = MeanShift().fit(X)\n",
    "        labels = ms_model.labels_\n",
    "\n",
    "    elif method == 'GMM':\n",
    "        X['RTA_count'] = minmax_scale(X['RTA_count'])\n",
    "        gmm = GaussianMixture(n_components=4, verbose=verbose, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        labels = gmm.predict(X)\n",
    "    else:\n",
    "        display('Select method \"MeanShift\" or \"GMM\"')\n",
    "        #return 'error'\n",
    "\n",
    "    labels = pd.DataFrame(labels,columns=['cluster'])\n",
    "    clustered_time_buckets = pd.concat([group_stats,labels], axis=1)\n",
    "\n",
    "    if verbose > 0:\n",
    "        display(clustered_time_buckets.groupby('cluster').agg({'RTA_count': ['count', np.sum]}))\n",
    "    if verbose > 1:\n",
    "        plot_TW_cluster(clustered_time_buckets)\n",
    "    \n",
    "    crash_df = crash_df.merge(clustered_time_buckets[['time_window_str', 'weekday','cluster']], how='left', on=['time_window_str', 'weekday'])\n",
    "    return crash_df\n",
    "\n",
    "def plot_TW_cluster(clustered_time_buckets):\n",
    "    '''\n",
    "    Displays stripplot to show how different times of the week are assigned to TW clusters.\n",
    "    '''\n",
    "    tb_clusters = sns.FacetGrid(clustered_time_buckets,hue='cluster', height=5)\n",
    "    tb_clusters.map(sns.stripplot,'weekday', 'time_window_str', s=25, order = ['00-03', '03-06', '06-09', '09-12', \n",
    "                                                                              '12-15', '15-18', '18-21', '21-24'])\n",
    "    \n",
    "def assign_TW_cluster(weekday, time_window, holiday=0, strategy='baseline'):\n",
    "    '''\n",
    "    Can be used in a lambda function to return the time window cluster for a given day and time window.\n",
    "    e.g. crash_df[\"cluster\"] = crash_df.apply(lambda x: return_TW_cluster(x.weekday, x.time_window_str) ,axis=1)\n",
    "    This is called by the function: create_cluster_feature.\n",
    "    '''\n",
    "    if strategy == 'baseline':\n",
    "        if holiday == 1:\n",
    "            return 'off_peak'        \n",
    "        elif weekday == 6:\n",
    "            return 'off_peak'\n",
    "        elif weekday in [0,1,2,3,4]:\n",
    "            if time_window in [\"06-09\"]:\n",
    "                return 'peak'\n",
    "            elif time_window in [\"09-12\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'    \n",
    "        elif weekday == 5:\n",
    "            if time_window in [\"06-09\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'\n",
    "            elif time_window in [\"09-12\"]:\n",
    "                return 'peak'\n",
    "    \n",
    "    elif strategy == 'saturday_2':\n",
    "        if holiday == 1:\n",
    "            return 'off_peak'        \n",
    "        elif weekday == 6:\n",
    "            return 'off_peak'\n",
    "        elif weekday in [0,1,2,3,4]:\n",
    "            if time_window in [\"06-09\"]:\n",
    "                return 'peak'\n",
    "            elif time_window in [\"09-12\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'middle'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'    \n",
    "        elif weekday == 5:\n",
    "            if time_window in [\"06-09\", \"12-15\", \"15-18\", \"18-21\"]:\n",
    "                return 'saturday_busy'\n",
    "            elif time_window in [\"00-03\", \"03-06\", \"21-24\"]:\n",
    "                return 'off_peak'\n",
    "            elif time_window in [\"09-12\"]:\n",
    "                return 'saturday_busy'         \n",
    "\n",
    "def create_cluster_feature(crash_df, strategy='baseline', verbose=0):\n",
    "    '''\n",
    "    Function takes crash df and creates new column with tw cluster labels.\n",
    "    If verbose is increased, the time window clusters will be visualised.\n",
    "    '''\n",
    "    crash_df[\"cluster\"] = crash_df.apply(lambda x: \n",
    "                                         assign_TW_cluster(weekday=x.weekday,\n",
    "                                                           time_window=x.time_window_str,\n",
    "                                                           strategy=strategy) \n",
    "                                         ,axis=1)\n",
    "    \n",
    "    print(f'{crash_df.cluster.nunique()} clusters created')\n",
    "    if verbose > 0:\n",
    "        tb_clusters = sns.FacetGrid(crash_df,hue='cluster', height=5)\n",
    "        tb_clusters.map(sns.stripplot,'weekday', 'time_window_str', s=25, \n",
    "                                       order = ['00-03', '03-06', '06-09', '09-12', \n",
    "                                                '12-15', '15-18', '18-21', '21-24'],\n",
    "                                    label = 'Time Window Clusters')\n",
    "    return crash_df\n",
    "            \n",
    "\n",
    "def create_baseline_submission_df(crash_data_df, date_start='2019-07-01', date_end='2020-01-01', method='k_means'):\n",
    "    '''Takes crash data and creates a data frame in the format needed for submission'''\n",
    "       \n",
    "    # star grid\n",
    "    if method == 'star':\n",
    "        lat_centroid = list(crash_data_df.latitude.quantile(q=[1/5,2/5,3/5,4/5]))\n",
    "        lon_centroid = list(crash_data_df.longitude.quantile(q=[1/4,2/4,3/4]))\n",
    "        centroids=[(lat_centroid[1],lon_centroid[0]),(lat_centroid[2],lon_centroid[0]),\n",
    "                   (lat_centroid[0],lon_centroid[1]),(lat_centroid[3],lon_centroid[1]),\n",
    "                   (lat_centroid[1],lon_centroid[2]),(lat_centroid[2],lon_centroid[2])]\n",
    "    \n",
    "    # Create centroids via k-means\n",
    "    if method == 'k_means':\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-3, max_iter=100, n_init=20, copy_x= True)\n",
    "        kmeans.fit(crash_data_df[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Create Date range covering submission period set\n",
    "    dates = pd.date_range(date_start, date_end, freq='3h')\n",
    "        \n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({'date':dates})\n",
    "    #submission_df['weekday'] = submission_df['date'].apply\n",
    "    for ambulance in range(6):\n",
    "        # Place an ambulance in the center of the city:\n",
    "        submission_df['A'+str(ambulance)+'_Latitude'] = centroids[ambulance][0]\n",
    "        submission_df['A'+str(ambulance)+'_Longitude'] = centroids[ambulance][1]\n",
    "    #print(f'Score={score()}')\n",
    "    return submission_df, centroids\n",
    "\n",
    "def create_k_means_centroids(crash_df_with_cluster, verbose=0):\n",
    "    centroids_dict = {}\n",
    "    for i in crash_df_with_cluster.cluster.unique():\n",
    "        kmeans = KMeans(n_clusters=6, verbose=0, tol=1e-4, max_iter=200, n_init=20)\n",
    "        kmeans.fit(crash_df_with_cluster.query('cluster==@i')[['latitude','longitude']])\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        centroids_dict[i] = centroids.flatten()\n",
    "        if verbose > 2:\n",
    "            plot_centroids(crash_df_with_cluster.query('cluster==@i'), centroids, cluster='cluster')\n",
    "    print(f'{len(centroids_dict)} centroids created')\n",
    "    return centroids_dict\n",
    "\n",
    "        \n",
    "def centroid_to_submission(centroids_dict, date_start='2019-07-01', date_end='2020-01-01'):\n",
    "    '''Takes dictionary of clusters and centroids and creates a data frame in the format needed for submission'''\n",
    "    \n",
    "    # Create Date range covering submission period set\n",
    "    dates = pd.date_range(date_start, date_end, freq='3h')\n",
    "    submission_df = pd.DataFrame({'date':dates})\n",
    "    submission_df = create_temporal_features(submission_df,'date')\n",
    "    submission_df[\"cluster\"] = submission_df.apply(lambda x: assign_TW_cluster(x.weekday, x.time_window_str) ,axis=1)\n",
    "    ambulance_columns = ['A0_Latitude', 'A0_Longitude', 'A1_Latitude','A1_Longitude', 'A2_Latitude', 'A2_Longitude', \n",
    "                         'A3_Latitude', 'A3_Longitude', 'A4_Latitude', 'A4_Longitude', 'A5_Latitude', 'A5_Longitude']\n",
    "\n",
    "    for i in submission_df[\"cluster\"].unique():\n",
    "        submission_df[\"placements\"] = submission_df[\"cluster\"].apply(lambda x: centroids_dict.get(x))\n",
    "        submission_df[ambulance_columns] = pd.DataFrame(submission_df.placements.tolist(), index= submission_df.index)\n",
    "    submission_df = submission_df.drop('placements', axis=1)\n",
    "    submission_df = drop_temporal(submission_df)\n",
    "    print('submission dataframe created')\n",
    "    return submission_df\n",
    "\n",
    "def create_submission_csv(submission_df, crash_source, tw_cluster_strategy, model_name, path='../Outputs/'):\n",
    "    '''Takes dataframe in submission format and outputs a csv file with matching name'''\n",
    "    current_time = datetime.datetime.now()\n",
    "    filename = f'{current_time.year}{current_time.month}{current_time.day}_{crash_source}_{tw_cluster_strategy}_{model_name}.csv'\n",
    "    submission_df.to_csv(path+filename,index=False)\n",
    "    print(f'{filename} saved in {path}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 clusters created\n",
      "4 centroids created\n",
      "submission dataframe created\n",
      "2020122_Train_saturday_2_k_means.csv saved in ../Outputs/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def ambulance_placement_pipeline(input_path='../Inputs/', output_path='../Outputs/', crash_source_csv='Train',\n",
    "                                 tw_cluster_strategy='saturday_2', placement_model='k_means', verbose=0):  \n",
    "    '''\n",
    "    load crash data (from train or prediction) and apply feautre engineering, run tw clustering (based on strategy choice) \n",
    "    create ambulance placements, create output file.\n",
    "    '''\n",
    "    crash_df = create_crash_df(train_file = input_path+crash_source_csv+'.csv')\n",
    "    crash_df = create_temporal_features(crash_df)\n",
    "    crash_df = create_cluster_feature(crash_df, strategy=tw_cluster_strategy, verbose=verbose)\n",
    "    centroids_dict = create_k_means_centroids(crash_df, verbose=verbose)\n",
    "    submission_df = centroid_to_submission(centroids_dict)\n",
    "    create_submission_csv(submission_df, crash_source=crash_source_csv, tw_cluster_strategy=tw_cluster_strategy, model_name=placement_model, path=output_path)\n",
    "    \n",
    "ambulance_placement_pipeline(input_path='../Inputs/', output_path='../Outputs/', \n",
    "                             crash_source_csv='Train', tw_cluster_strategy='saturday_2', \n",
    "                             placement_model='k_means', verbose=0)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nairobi_ambulance] *",
   "language": "python",
   "name": "conda-env-nairobi_ambulance-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
